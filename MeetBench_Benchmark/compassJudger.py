from transformers import AutoModelForCausalLM, AutoTokenizer


model_name = "opencompass/CompassJudger-1-7B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """你是一个擅长评价文本质量的助手。\n请你以公正的评判者的身份，评估一个AI助手对于用户提问的回答的质量。由于您评估的回答类型是角色扮演，因此你需要从下面的几个维度对回答进行评估:
1. 事实正确性: 回答中提供的信息是否准确无误，是否基于可信的事实和数据。
2. 满足用户需求: 回答是否满足了用户提出问题的目的和需求，是否对问题进行了全面而恰当的回应。
3. 逻辑连贯性: 回答是否在整体上保持一致，是否在不同部分之间保持逻辑连贯性，避免了自相矛盾。
4. 创造性: 回答是否具有创新性或独特性，是否提供了新颖的见解或解决方法。
5. 丰富度: 回答包含丰富的信息、深度、上下文考虑、多样性、详细解释和实例，以满足用户需求并提供全面理解。
我们会给您提供用户的提问，高质量的参考答案，和需要你评估的AI助手的答案。
当你开始你的评估时，你需要按照遵守以下的流程：
1. 将AI助手的答案与参考答案进行比较，指出AI助手的答案有哪些不足，并进一步解释。
2. 从不同维度对AI助手的答案进行评价，在每个维度的评价之后，给每一个维度一个1～10的分数。
3. 最后，综合每个维度的评估，对AI助手的回答给出一个1～10的综合分数。
4. 你的打分需要尽可能严格，并且要遵守下面的评分规则：
总的来说，模型回答的质量越高，则分数越高。
其中，事实正确性和满足用户需求这两个维度是最重要的，这两个维度的分数主导了最后的综合分数。当模型回答存在与问题不相关，或者有本质性的事实错误，或生成了有害内容时，总分必须是1到2分；当模型回答没有严重错误而且基本无害，但是质量较低，没有满足用户需求，总分为3到4分；当模型回答基本满足用户要求，但是在部分维度上表现较差，质量中等，总分可以得5到6分；当模型回答质量与参考答案相近，在所有维度上表现良好，总分得7到8分；只有当模型回答质量显著超过参考答案，充分地解决了用户问题和所有需求，并且在所有维度上都接近满分的情况下，才能得9到10分。作为示例，参考答案可以得到8分。
请记住，你必须在你打分前进行评价和解释。在你对每个维度的解释之后，需要加上对该维度的打分。之后，在你回答的末尾，按照以下字典格式（包括括号）返回你所有的打分结果，并确保你的打分结果是整数：
{'维度一': 打分, '维度二': 打分, ..., '综合得分': 打分}，例如：{'事实正确性': 9, '满足用户需求': 6, ..., '综合得分': 7}。
用户的提问： 交交针对顾客反映价格高的问题公司打算如何凭成本和价格
[参考答案开始]\n对于顾客反映价格高的问题，公司可能会采取多种策略来平衡成本和价格。首先，公司可以努力提高生产效率和优化供应链，以降低生产成本。其次，公司可以考虑提供不同价格的产品选项，以满足不同消费能力的顾客需求。此外，通过提供优质的售后服务和产品保障，增加产品的附加价值，也许可以使顾客接受较高的价格。总的来说，公司需要在保持盈利的同时，不断提升产品和服务质量，以满足顾客的需求。\n[参考答案结束]
\n[助手的答案开始]\n公司打算通过提高生产效率，降低原材料成本以及优化管理流程来解决价格高的问题。同时，也会考虑调整产品结构，推出更多性价比更高的产品以吸引消费者。\n[助手的答案结束]"""



messages = [
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2048
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)